{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qq7NcjONFIef"
   },
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Parallel Join Strategies](#parallel-join)\n",
    "    * [Broadcast Hash Join](#bhj)\n",
    "    * [Sort Merge Join](#smj)\n",
    "* [Parallel Joins](#other-joins)\n",
    "    * [Inner Join](#inner)        \n",
    "    * [Left Join](#left)        \n",
    "    * [Full Outer Join](#full_outer)        \n",
    "    * [Left Semi Join](#left_semi)        \n",
    "    * [Left Anti Join](#left_anti)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Spark classes and create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "master = \"local[*]\"\n",
    "app_name = \"Parallel Join\"\n",
    "\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Join Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|valueA|\n",
      "+---+------+\n",
      "|107|  A107|\n",
      "|108|  A108|\n",
      "|101|  A101|\n",
      "|105|  A105|\n",
      "|103|  A103|\n",
      "|104|  A104|\n",
      "|102|  A102|\n",
      "|100|  A100|\n",
      "|109|  A109|\n",
      "|106|  A106|\n",
      "+---+------+\n",
      "\n",
      "+---+------+\n",
      "| id|valueB|\n",
      "+---+------+\n",
      "|450|  B450|\n",
      "|633|  B633|\n",
      "|170|  B170|\n",
      "|561|  B561|\n",
      "|247|  B247|\n",
      "|400|  B400|\n",
      "|590|  B590|\n",
      "|290|  B290|\n",
      "|134|  B134|\n",
      "|739|  B739|\n",
      "|991|  B991|\n",
      "|504|  B504|\n",
      "|241|  B241|\n",
      "|311|  B311|\n",
      "|964|  B964|\n",
      "|669|  B669|\n",
      "|928|  B928|\n",
      "|831|  B831|\n",
      "|270|  B270|\n",
      "|119|  B119|\n",
      "+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Setting dataset\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# List of tuples\n",
    "tableA = [(i,'A'+str(i)) for i in range(100,110)]\n",
    "tableB = [(i,'B'+str(i)) for i in range(10,1000)]\n",
    "\n",
    "# Shuffle the lists to not have it ordered\n",
    "random.shuffle(tableA)\n",
    "random.shuffle(tableB)\n",
    "\n",
    "# Converting to dataframe each list of tuples\n",
    "df_A = spark.createDataFrame(tableA , [\"id\", \"valueA\"])\n",
    "df_A.show()\n",
    "df_B = spark.createDataFrame(tableB , [\"id\", \"valueB\"])\n",
    "df_B.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Broadcast Hash Join\n",
    "In this type of join, one dataset(the smaller one) is broadcasted (sent over) to each executor. By doing this, we can avoid the shuffle for the other larger dataset. Not doing the shuffle increase the speed of the join operation.\n",
    "\n",
    "<i>We need to use the broadcast function inside the join to broadcast the table</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|valueB| id|valueA|\n",
      "+---+------+---+------+\n",
      "|108|  B108|108|  A108|\n",
      "|104|  B104|104|  A104|\n",
      "|100|  B100|100|  A100|\n",
      "|102|  B102|102|  A102|\n",
      "|106|  B106|106|  A106|\n",
      "|109|  B109|109|  A109|\n",
      "|101|  B101|101|  A101|\n",
      "|103|  B103|103|  A103|\n",
      "|105|  B105|105|  A105|\n",
      "|107|  B107|107|  A107|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Use broadcast function to specify the use of BroadcastHashJoin algorithm\n",
    "df_joined_broadcast = df_B.join(broadcast(df_A),df_A.id==df_B.id,how='inner')\n",
    "df_joined_broadcast.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation Query Plan with Broadcast Hash Join\n",
    "The order of execution goes from top to bottom. The steps are:\n",
    "1. Scan dataframe A (left side)\n",
    "  - Filter id not null in dataframe A\n",
    "2. Scan dataframe B (right side)\n",
    "  - Filter id not null in dataframe B\n",
    "3. Broadcast dataframe B: Send dataframe B to each each partition\n",
    "4. BroadcastHashJoin: Perform join between each partition and the broadcasted dataframe B\n",
    "5. Project: Select the attributes from both dataframes (df_A: id,valueA and df_b: id,valueB)\n",
    "6. Collect all the results to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sort Merge Join\n",
    "In this join approach, the datasets are sorted first and the second operation merges the sorted data in the partition. This is the <strong>default</strong> join algorithm used by spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|valueA| id|valueB|\n",
      "+---+------+---+------+\n",
      "|107|  A107|107|  B107|\n",
      "|108|  A108|108|  B108|\n",
      "|101|  A101|101|  B101|\n",
      "|103|  A103|103|  B103|\n",
      "|105|  A105|105|  B105|\n",
      "|104|  A104|104|  B104|\n",
      "|102|  A102|102|  B102|\n",
      "|100|  A100|100|  B100|\n",
      "|106|  A106|106|  B106|\n",
      "|109|  A109|109|  B109|\n",
      "+---+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_sortmerge = df_A.join(df_B,df_A.id==df_B.id,how='inner')\n",
    "df_joined_sortmerge.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation Query Plan with Sort Merge Join\n",
    "The order of execution goes from top to bottom. The steps are:\n",
    "1. Scan dataframe A (left side)\n",
    "  - Filter id not null in dataframe A\n",
    "2. Scan dataframe B (right side)\n",
    "  - Filter id not null in dataframe B\n",
    "3. Exchange dataframe A: Partition dataframe A with hash partitioning\n",
    "4. Exchange dataframe B: Partition dataframe B with hash partitioning\n",
    "5. Sort dataframe A: Sort data within each partition\n",
    "6. Sort dataframe B: Sort data within each partition\n",
    "7. Perform Sort Merge Join between both dataframes\n",
    "5. Project: Select the attributes from both dataframes (df_A: id,valueA and df_b: id,valueB)\n",
    "6. Collect all the results to the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Join\n",
    "\n",
    "Now we will implement multiple join operations and visualise the parallelism embedded in Spark to perform these kind of queries. The join queries that we will perform are:\n",
    "1. Inner Join\n",
    "1. Left Join\n",
    "1. Full Outer Join\n",
    "1. Left Semi Join\n",
    "1. Left Anti Join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Dictionary Schema\n",
    "schema_dictionary = StructType([\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Code\", StringType(), True),\n",
    "    StructField(\"Population\", StringType(), True),  # Assuming Population as string to match your structure; you might want it as IntegerType\n",
    "    StructField(\"GDP per Capita\", StringType(), True),  # Assuming GDP per Capita as string; consider FloatType for actual calculations\n",
    "])\n",
    "\n",
    "# Summer Schema\n",
    "schema_summer = StructType([\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Sport\", StringType(), True),\n",
    "    StructField(\"Discipline\", StringType(), True),\n",
    "    StructField(\"Athlete\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "    StructField(\"Medal\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Winter Schema\n",
    "schema_winter = StructType([\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Sport\", StringType(), True),\n",
    "    StructField(\"Discipline\", StringType(), True),\n",
    "    StructField(\"Athlete\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "    StructField(\"Medal\", StringType(), True),\n",
    "])\n",
    "\n",
    "data_dictionary = [\n",
    "    (\"United States\", \"USA\", \"331002651\", \"59939\"),\n",
    "    (\"China\", \"CHN\", \"1439323776\", \"10416\"),\n",
    "    (\"Japan\", \"JPN\", \"126476461\", \"39434\"),\n",
    "]\n",
    "\n",
    "# Sample Data for Summer Olympics\n",
    "data_summer = [\n",
    "    (\"2020\", \"Tokyo\", \"Swimming\", \"100m Freestyle\", \"John Doe\", \"USA\", \"M\", \"100m Freestyle\", \"Gold\"),\n",
    "    (\"2020\", \"Tokyo\", \"Athletics\", \"Marathon\", \"Jane Doe\", \"KEN\", \"F\", \"Marathon\", \"Silver\"),\n",
    "]\n",
    "\n",
    "# Sample Data for Winter Olympics\n",
    "data_winter = [\n",
    "    (\"2018\", \"Pyeongchang\", \"Skiing\", \"Downhill\", \"Alex Smith\", \"USA\", \"M\", \"Downhill\", \"Bronze\"),\n",
    "    (\"2018\", \"Pyeongchang\", \"Skating\", \"Figure Skating\", \"Emily White\", \"CAN\", \"F\", \"Singles\", \"Gold\"),\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "df_dictionary = spark.createDataFrame(data=data_dictionary, schema=schema_dictionary)\n",
    "df_summer = spark.createDataFrame(data=data_summer, schema=schema_summer)\n",
    "df_winter = spark.createDataFrame(data=data_winter, schema=schema_winter)\n",
    "\n",
    "# Repartition Summer and Winter DataFrames as described\n",
    "df_summer = df_summer.repartition(4)\n",
    "df_winter = df_winter.repartition(4)\n",
    "\n",
    "df_dictionary.createOrReplaceTempView(\"sql_dictionary\")\n",
    "df_summer.createOrReplaceTempView(\"sql_summer\")\n",
    "df_winter.createOrReplaceTempView(\"sql_winter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICTIONARY INFO:\n",
      "Number of partitions: 8\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Population: string (nullable = true)\n",
      " |-- GDP per Capita: string (nullable = true)\n",
      "\n",
      "SUMMER INFO:\n",
      "Number of partitions: 4\n",
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Discipline: string (nullable = true)\n",
      " |-- Athlete: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Medal: string (nullable = true)\n",
      "\n",
      "WINTER INFO:\n",
      "Number of partitions: 4\n",
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Discipline: string (nullable = true)\n",
      " |-- Athlete: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Medal: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Verifying the number of partitions for each dataframe\n",
    "## You can explore the data of each csv file with the function printSchema()\n",
    "print(f\"DICTIONARY INFO:\")\n",
    "print(f\"Number of partitions: {df_dictionary.rdd.getNumPartitions()}\")\n",
    "df_dictionary.printSchema()\n",
    "      \n",
    "print(f\"SUMMER INFO:\")\n",
    "print(f\"Number of partitions: {df_summer.rdd.getNumPartitions()}\")\n",
    "df_summer.printSchema()\n",
    "      \n",
    "print(f\"WINTER INFO:\")\n",
    "print(f\"Number of partitions: {df_winter.rdd.getNumPartitions()}\")\n",
    "df_winter.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inner Join\n",
    "This join operation returns the result set that have matching values in both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|      Country|Code|Population|GDP per Capita|Year| City|   Sport|    Discipline| Athlete|Country|Gender|         Event|Medal|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|United States| USA| 331002651|         59939|2020|Tokyo|Swimming|100m Freestyle|John Doe|    USA|     M|100m Freestyle| Gold|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "\n",
      "1\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|      Country|Code|Population|GDP per Capita|Year| City|   Sport|    Discipline| Athlete|Country|Gender|         Event|Medal|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|United States| USA| 331002651|         59939|2020|Tokyo|Swimming|100m Freestyle|John Doe|    USA|     M|100m Freestyle| Gold|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Join summer and dictionary using Dataframes\n",
    "df_dict_inner_summ = df_dictionary.join(df_summer,df_dictionary.Code==df_summer.Country,how='inner')\n",
    "print(df_dict_inner_summ.count())\n",
    "df_dict_inner_summ.show()\n",
    "\n",
    "## Join summer and dictionary using SQL\n",
    "sql_dict_inner_summ = spark.sql('''\n",
    "  SELECT d.*,w.*\n",
    "  FROM sql_dictionary d JOIN sql_summer w\n",
    "  ON d.Code=w.Country\n",
    "''')\n",
    "print(sql_dict_inner_summ.count())\n",
    "sql_dict_inner_summ.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Left Join\n",
    "This join operation returns all records from the left dataframe and the matched records from the right dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|      Country|Code|Population|GDP per Capita|Year| City|   Sport|    Discipline| Athlete|Country|Gender|         Event|Medal|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|        China| CHN|1439323776|         10416|NULL| NULL|    NULL|          NULL|    NULL|   NULL|  NULL|          NULL| NULL|\n",
      "|        Japan| JPN| 126476461|         39434|NULL| NULL|    NULL|          NULL|    NULL|   NULL|  NULL|          NULL| NULL|\n",
      "|United States| USA| 331002651|         59939|2020|Tokyo|Swimming|100m Freestyle|John Doe|    USA|     M|100m Freestyle| Gold|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "\n",
      "3\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|      Country|Code|Population|GDP per Capita|Year| City|   Sport|    Discipline| Athlete|Country|Gender|         Event|Medal|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "|        China| CHN|1439323776|         10416|NULL| NULL|    NULL|          NULL|    NULL|   NULL|  NULL|          NULL| NULL|\n",
      "|        Japan| JPN| 126476461|         39434|NULL| NULL|    NULL|          NULL|    NULL|   NULL|  NULL|          NULL| NULL|\n",
      "|United States| USA| 331002651|         59939|2020|Tokyo|Swimming|100m Freestyle|John Doe|    USA|     M|100m Freestyle| Gold|\n",
      "+-------------+----+----------+--------------+----+-----+--------+--------------+--------+-------+------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#### Join summer and dictionary using Dataframes\n",
    "df_dict_left_summ = df_dictionary.join(df_summer,df_dictionary.Code==df_summer.Country,how='left')\n",
    "# df_dict_inner_summ = df_dict_inner_summ.filter(col('Discipline').isNull())\n",
    "print(df_dict_left_summ.count())\n",
    "df_dict_left_summ.show()\n",
    "\n",
    "## Join summer and dictionary using SQL\n",
    "sql_dict_left_summ = spark.sql('''\n",
    "  SELECT d.*,w.*\n",
    "  FROM sql_dictionary d LEFT JOIN sql_summer w\n",
    "  ON d.Code=w.Country\n",
    "''')\n",
    "print(sql_dict_left_summ.count())\n",
    "sql_dict_left_summ.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Full Outer Join\n",
    "This join operation returns a result set that includes rows from both left and right dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "+-------------+----+----------+--------------+----+-----+---------+--------------+--------+-------+------+--------------+------+\n",
      "|      Country|Code|Population|GDP per Capita|Year| City|    Sport|    Discipline| Athlete|Country|Gender|         Event| Medal|\n",
      "+-------------+----+----------+--------------+----+-----+---------+--------------+--------+-------+------+--------------+------+\n",
      "|        China| CHN|1439323776|         10416|NULL| NULL|     NULL|          NULL|    NULL|   NULL|  NULL|          NULL|  NULL|\n",
      "|        Japan| JPN| 126476461|         39434|NULL| NULL|     NULL|          NULL|    NULL|   NULL|  NULL|          NULL|  NULL|\n",
      "|         NULL|NULL|      NULL|          NULL|2020|Tokyo|Athletics|      Marathon|Jane Doe|    KEN|     F|      Marathon|Silver|\n",
      "|United States| USA| 331002651|         59939|2020|Tokyo| Swimming|100m Freestyle|John Doe|    USA|     M|100m Freestyle|  Gold|\n",
      "+-------------+----+----------+--------------+----+-----+---------+--------------+--------+-------+------+--------------+------+\n",
      "\n",
      "4\n",
      "+-------------+----+----------+--------------+----+-----+---------+--------------+--------+-------+------+--------------+------+\n",
      "|      Country|Code|Population|GDP per Capita|Year| City|    Sport|    Discipline| Athlete|Country|Gender|         Event| Medal|\n",
      "+-------------+----+----------+--------------+----+-----+---------+--------------+--------+-------+------+--------------+------+\n",
      "|        China| CHN|1439323776|         10416|NULL| NULL|     NULL|          NULL|    NULL|   NULL|  NULL|          NULL|  NULL|\n",
      "|        Japan| JPN| 126476461|         39434|NULL| NULL|     NULL|          NULL|    NULL|   NULL|  NULL|          NULL|  NULL|\n",
      "|         NULL|NULL|      NULL|          NULL|2020|Tokyo|Athletics|      Marathon|Jane Doe|    KEN|     F|      Marathon|Silver|\n",
      "|United States| USA| 331002651|         59939|2020|Tokyo| Swimming|100m Freestyle|John Doe|    USA|     M|100m Freestyle|  Gold|\n",
      "+-------------+----+----------+--------------+----+-----+---------+--------------+--------+-------+------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Join summer and dictionary using Dataframes\n",
    "df_dict_outer_summ = df_dictionary.join(df_summer,df_dictionary.Code==df_summer.Country,how='outer')\n",
    "print(df_dict_outer_summ.count())\n",
    "df_dict_outer_summ.show()\n",
    "\n",
    "## Join summer and dictionary using SQL\n",
    "sql_dict_outer_summ = spark.sql('''\n",
    "  SELECT d.*,w.*\n",
    "  FROM sql_dictionary d FULL OUTER JOIN sql_summer w\n",
    "  ON d.Code=w.Country\n",
    "''')\n",
    "print(sql_dict_outer_summ.count())\n",
    "sql_dict_outer_summ.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Left Semi Join\n",
    "This join operation is like an inner join, but only the left dataframe columns and values are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "+-------------+----+----------+--------------+\n",
      "|      Country|Code|Population|GDP per Capita|\n",
      "+-------------+----+----------+--------------+\n",
      "|United States| USA| 331002651|         59939|\n",
      "+-------------+----+----------+--------------+\n",
      "\n",
      "1\n",
      "+-------------+----+----------+--------------+\n",
      "|      Country|Code|Population|GDP per Capita|\n",
      "+-------------+----+----------+--------------+\n",
      "|United States| USA| 331002651|         59939|\n",
      "+-------------+----+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Join summer and dictionary using Dataframes\n",
    "df_dict_semi_summ = df_dictionary.join(df_summer,df_dictionary.Code==df_summer.Country,how='left_semi')\n",
    "print(df_dict_semi_summ.count())\n",
    "df_dict_semi_summ.show()\n",
    "\n",
    "sql_dict_outer_summ = spark.sql('''\n",
    "  SELECT d.*\n",
    "  FROM sql_dictionary d LEFT SEMI JOIN sql_summer w\n",
    "  ON d.Code=w.Country\n",
    "''')\n",
    "print(sql_dict_outer_summ.count())\n",
    "sql_dict_outer_summ.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Left Anti Join\n",
    "This join operation is the difference of the left dataframe minus the right dataframe, as it selects all rows from df1 that are not present in df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+-------+----+----------+--------------+\n",
      "|Country|Code|Population|GDP per Capita|\n",
      "+-------+----+----------+--------------+\n",
      "|  China| CHN|1439323776|         10416|\n",
      "|  Japan| JPN| 126476461|         39434|\n",
      "+-------+----+----------+--------------+\n",
      "\n",
      "2\n",
      "+-------+----+----------+--------------+\n",
      "|Country|Code|Population|GDP per Capita|\n",
      "+-------+----+----------+--------------+\n",
      "|  China| CHN|1439323776|         10416|\n",
      "|  Japan| JPN| 126476461|         39434|\n",
      "+-------+----+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Join summer and dictionary using Dataframes\n",
    "df_dict_anti_summ = df_dictionary.join(df_summer,df_dictionary.Code==df_summer.Country,how='left_anti')\n",
    "print(df_dict_anti_summ.count())\n",
    "df_dict_anti_summ.show()\n",
    "\n",
    "sql_dict_outer_summ = spark.sql('''\n",
    "  SELECT d.*\n",
    "  FROM sql_dictionary d LEFT ANTI JOIN sql_summer w\n",
    "  ON d.Code=w.Country\n",
    "''')\n",
    "print(sql_dict_outer_summ.count())\n",
    "sql_dict_outer_summ.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "FIT5202 - Getting started with Apache Spark.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
