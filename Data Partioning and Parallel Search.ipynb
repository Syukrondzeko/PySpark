{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext and SparkSession <a class=\"anchor\" name=\"one\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "master = \"local[*]\"\n",
    "app_name = \"Parallel Search\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agenda:\n",
    "1. Data Partitioning\n",
    "2. RDD Partitioning\n",
    "3. Parallel Search in RDD\n",
    "4. Spark DataFrame\n",
    "5. Parallel Search in Spark DataFrame\n",
    "6. Parallel Search using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "\n",
    "#### 1. Round-robin data partitioning ###\n",
    "Round-robin data partitioning is the simplest data partitioning method in which each record in turn is allocated to a processing element (simply processor). Since it distributes the data evenly among all processors, it is also known as \"equal-partitioning\".\n",
    "\n",
    "#### 2. Range data partitioning ###\n",
    "Range data partitioning records based on a given range of the partitioning attribute. For example,the student table is partitioned based on \"Last Name\" based on the alphabetical order (i.e. A ~ Z). \n",
    "\n",
    "#### 3. Hash data partitioning ###\n",
    "Hash data partitioning makes a partition based on a particular attribute using a hash function. The result of a hash function determines the processor where the record will be placed. Thus, all records within a partition have the same hash value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD partitioning <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "By default, Spark partitions the data using <strong>Random equal partitioning</strong> unless there are specific transformations that uses a different type of partitioning</strong>\n",
    "In the code below, we have defined two functions to implement custom partitioning using <strong>Range Partitioning</strong> and <strong>Hash Partitioning</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    print(f\"NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "            print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_players = [(1,'Ronaldo'),(2,'Messi'),(3,'Modric'),(4,'Xavi'),(5,'Iniesta'),\n",
    "                (10,'Kroos'),(11,'Bale'),(12, 'Benzema'),(3, 'Valverde'),(18,'Bellingham'),(9,'Carvajal')]\n",
    "\n",
    "#Define the number of partitions\n",
    "no_of_partitions = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Partitioning in Spark RDD <a class=\"anchor\" id=\"default\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random equal partition\n",
    "rdd = sc.parallelize(list_players, no_of_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:4\n",
      "Partitioner:None\n",
      "NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 2 records\n",
      "[(1, 'Ronaldo'), (2, 'Messi')]\n",
      "Partition 1: 4 records\n",
      "[(3, 'Modric'), (4, 'Xavi'), (5, 'Iniesta'), (10, 'Kroos')]\n",
      "Partition 2: 2 records\n",
      "[(11, 'Bale'), (12, 'Benzema')]\n",
      "Partition 3: 3 records\n",
      "[(3, 'Valverde'), (18, 'Bellingham'), (9, 'Carvajal')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions:{}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner:{}\".format(rdd.partitioner))\n",
    "print_partitions(rdd)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Partitioning in RDD <a class=\"anchor\" id=\"hash\"></a>\n",
    "Hash partitioning uses the formula <code>partition = hash_function() % numPartitions</code> to determine which partition data item falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hash Function to implement Hash Partitioning \n",
    "#Just computes the sum of digits\n",
    "#Example : hash_function(12) produces 3 i.e. 2 + 1\n",
    "#Then hash_function(12) % numPartitions = 3%4 = 3\n",
    "\n",
    "def hash_function(key):\n",
    "    total = 0\n",
    "    for digit in str(key):\n",
    "        total += int(digit)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 1 records\n",
      "[(4, 'Xavi')]\n",
      "Partition 1: 5 records\n",
      "[(1, 'Ronaldo'), (5, 'Iniesta'), (10, 'Kroos'), (18, 'Bellingham'), (9, 'Carvajal')]\n",
      "Partition 2: 2 records\n",
      "[(2, 'Messi'), (11, 'Bale')]\n",
      "Partition 3: 3 records\n",
      "[(3, 'Modric'), (12, 'Benzema'), (3, 'Valverde')]\n"
     ]
    }
   ],
   "source": [
    "# hash partitioning\n",
    "hash_partitioned_rdd = rdd.partitionBy(no_of_partitions, hash_function)\n",
    "print_partitions(hash_partitioned_rdd)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range Partitioning in RDD <a class=\"anchor\" id=\"range\"></a>\n",
    "This strategy uses a range to distribute the items to respective partitions when the keys fall within the range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_partitions=4\n",
    "\n",
    "chunk_size = len(list_players)/no_of_partitions\n",
    "range_arr=[[1,4],[5,9],[10,14],[15,19]]\n",
    "\n",
    "def range_function(key):\n",
    "    for index,item in enumerate(range_arr):\n",
    "        if key >=item[0] and key <=item[1]:\n",
    "            return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 5 records\n",
      "[(1, 'Ronaldo'), (2, 'Messi'), (3, 'Modric'), (4, 'Xavi'), (3, 'Valverde')]\n",
      "Partition 1: 2 records\n",
      "[(5, 'Iniesta'), (9, 'Carvajal')]\n",
      "Partition 2: 3 records\n",
      "[(10, 'Kroos'), (11, 'Bale'), (12, 'Benzema')]\n",
      "Partition 3: 1 records\n",
      "[(18, 'Bellingham')]\n"
     ]
    }
   ],
   "source": [
    "# range partition\n",
    "range_partitioned_rdd = rdd.partitionBy(no_of_partitions, range_function)\n",
    "print_partitions(range_partitioned_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Search using RDDs  <a class=\"anchor\" id=\"parallel-search-rdd\"></a>\n",
    "\n",
    "Now we will implement basic search functionalities and visualise the parallelism embedded in Spark to perform these kind of queries.\n",
    "\n",
    "In this tutorial, you will use a csv dataset **bank.csv**. However, for this tutorial we won't analyse the case study but only perform some search queries with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 2\n",
      "Number of lines: 11163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['age,job,marital,education,default,balance,housing,loan,contact,day,month,duration,campaign,pdays,previous,poutcome,deposit',\n",
       " '59,admin.,married,secondary,no,2343,yes,no,unknown,5,may,1042,1,-1,0,unknown,yes',\n",
       " '56,admin.,married,secondary,no,45,no,no,unknown,5,may,1467,1,-1,0,unknown,yes',\n",
       " '41,technician,married,secondary,no,1270,yes,no,unknown,5,may,1389,1,-1,0,unknown,yes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_rdd = sc.textFile('bank.csv')\n",
    "\n",
    "print(f\"Total partitions: {bank_rdd.getNumPartitions()}\")\n",
    "print(f\"Number of lines: {bank_rdd.count()}\")\n",
    "\n",
    "bank_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in RDDs based on multiple conditions\n",
    "\n",
    "We will focus on only four attributes from the data: age, education, marital and balance for filtering conditions. However, we will display additional information as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# Split each line separated by comma into a list \n",
    "bank_rdd1 = bank_rdd.map(lambda line: line.split(','))\n",
    "\n",
    "# Remove the header\n",
    "header = bank_rdd1.first()\n",
    "bank_rdd1 = bank_rdd1.filter(lambda row: row != header)   #filter out header\n",
    "\n",
    "# Indices for each attribute we will use\n",
    "# Filter: age, education, marital, balance = 0, 3, 2, 5\n",
    "# Display additional: day, month, deposit = 9, 10, 16\n",
    "\n",
    "bank_rdd1 = bank_rdd1.filter(lambda x: int(x[5])>1000 and int(x[5])<2000)\n",
    "bank_rdd1 = bank_rdd1.filter(lambda x: x[3] in ['primary','secondary'] and int(x[0])<30)\n",
    "bank_rdd1 = bank_rdd1.filter(lambda x: x[2]=='married' )\n",
    "bank_rdd1 = bank_rdd1.map(lambda field: (field[0],field[2],field[3],field[5],\n",
    "                                         field[9],field[10],field[16]))\n",
    "print(bank_rdd1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 2\n",
      "Partition 0:\n",
      "('29', 'married', 'secondary', '1135', '17', 'feb', 'yes')\n",
      "('27', 'married', 'secondary', '1293', '8', 'apr', 'yes')\n",
      "('29', 'married', 'secondary', '1180', '17', 'apr', 'yes')\n",
      "('28', 'married', 'secondary', '1086', '20', 'apr', 'yes')\n",
      "('26', 'married', 'secondary', '1595', '15', 'jun', 'yes')\n",
      "('27', 'married', 'secondary', '1596', '1', 'sep', 'yes')\n",
      "('28', 'married', 'secondary', '1595', '9', 'sep', 'yes')\n",
      "('27', 'married', 'secondary', '1595', '29', 'dec', 'yes')\n",
      "Partition 1:\n",
      "('26', 'married', 'secondary', '1417', '6', 'jun', 'no')\n",
      "('23', 'married', 'secondary', '1309', '3', 'jun', 'no')\n",
      "('24', 'married', 'secondary', '1222', '20', 'apr', 'no')\n",
      "('28', 'married', 'secondary', '1238', '14', 'may', 'no')\n",
      "('26', 'married', 'secondary', '1595', '2', 'mar', 'no')\n",
      "('27', 'married', 'secondary', '1303', '21', 'may', 'no')\n",
      "('25', 'married', 'secondary', '1782', '19', 'jun', 'no')\n",
      "('28', 'married', 'secondary', '1137', '6', 'feb', 'no')\n",
      "('28', 'married', 'secondary', '1020', '28', 'may', 'no')\n",
      "('29', 'married', 'secondary', '1386', '28', 'may', 'no')\n"
     ]
    }
   ],
   "source": [
    "numPartitions = bank_rdd1.getNumPartitions()\n",
    "print(f\"Total partitions: {numPartitions}\")\n",
    "\n",
    "# glom(): Return an RDD created by coalescing all elements within each partition into a list\n",
    "partitions = bank_rdd1.glom().collect()\n",
    "for index,partition in enumerate(partitions):\n",
    "    print(f'Partition {index}:')\n",
    "    for record in partition:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching max/min value of an attribute in an RDD\n",
    "This task will aim to find the record in the dataset that contains the highest value for a given attribute. In this case the attribute chosen is \"balance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv but now with 4 partitions\n",
    "bank_rdd_4 = sc.textFile('bank.csv',4)\n",
    "\n",
    "# Split and remove the header\n",
    "bank_rdd_4 = bank_rdd_4.map(lambda line: line.split(','))\n",
    "header = bank_rdd_4.first()\n",
    "bank_rdd_4 = bank_rdd_4.filter(lambda row: row != header)   #filter out header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['58', 'self-employed', 'married', 'secondary', 'no', '9994', 'no', 'no', 'cellular', '10', 'jul', '400', '1', '-1', '0', 'unknown', 'no']\n"
     ]
    }
   ],
   "source": [
    "# Get max by value in index 5 (balance)\n",
    "\n",
    "# Wrong way\n",
    "result_max_balance = bank_rdd_4.max(key=lambda x: x[5]) \n",
    "print(result_max_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['84', 'retired', 'married', 'secondary', 'no', '81204', 'no', 'no', 'telephone', '28', 'dec', '679', '1', '313', '2', 'other', 'yes']\n",
      "['84', 'retired', 'married', 'secondary', 'no', '81204', 'no', 'no', 'telephone', '28', 'dec', '679', '1', '313', '2', 'other', 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Correct way\n",
    "result_max_balance2 = bank_rdd_4.takeOrdered(1,lambda x: -1*int(x[5]))[0]\n",
    "print(result_max_balance2)\n",
    "\n",
    "result_max_balance3 = bank_rdd_4.reduce(lambda x,y: y if int(y[5])>int(x[5]) else x)\n",
    "print(result_max_balance3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames in Spark <a class=\"anchor\" id=\"dataframes\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------+\n",
      "| Id|   Name|Initial|\n",
      "+---+-------+-------+\n",
      "|  1|Ronaldo|      R|\n",
      "|  2|  Messi|      M|\n",
      "|  3| Modric|      M|\n",
      "|  4|   Xavi|      X|\n",
      "|  5|Iniesta|      I|\n",
      "+---+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Initial: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,'Ronaldo', 'R'),(2,'Messi', 'M'),(3,'Modric', 'M'),(4,'Xavi', 'X'),(5,'Iniesta', 'I'),\n",
    "                            (10,'Kroos', 'K'),(11,'Bale', 'B'),(12, 'Benzema', 'B'),(3, 'Valverde', 'V'),(18,'Bellingham', 'B'),(9,'Carvajal', 'C')],\n",
    "                           ['Id','Name','Initial'])\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to create a DataFrame is use the <strong>spark.read.csv</strong> file to load the data from csv to a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning in DataFrames <a class=\"anchor\" id=\"df-partitioning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round-robin data partitioning\n",
    "df_round = df.repartition(2)\n",
    "\n",
    "# Range data partitioning\n",
    "df_range = df.repartitionByRange(2, \"Initial\")\n",
    "\n",
    "# Hash data partitioning\n",
    "column_hash = \"Id\"\n",
    "df_hash = df.repartition(column_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Search using Spark Dataframe <a class=\"anchor\" id=\"parallel_search_df\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"bank.csv\",header=True)\n",
    "bank_df = df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+---+-----+-------+\n",
      "|age|education|balance|day|month|deposit|\n",
      "+---+---------+-------+---+-----+-------+\n",
      "| 27|secondary|   1293|  8|  apr|    yes|\n",
      "| 26|secondary|   1595| 15|  jun|    yes|\n",
      "| 28|secondary|   1595|  9|  sep|    yes|\n",
      "| 28|secondary|   1020| 28|  may|     no|\n",
      "| 27|secondary|   1303| 21|  may|     no|\n",
      "| 28|secondary|   1086| 20|  apr|    yes|\n",
      "| 23|secondary|   1309|  3|  jun|     no|\n",
      "| 29|secondary|   1180| 17|  apr|    yes|\n",
      "| 26|secondary|   1595|  2|  mar|     no|\n",
      "| 26|secondary|   1417|  6|  jun|     no|\n",
      "| 27|secondary|   1595| 29|  dec|    yes|\n",
      "| 24|secondary|   1222| 20|  apr|     no|\n",
      "| 28|secondary|   1137|  6|  feb|     no|\n",
      "| 25|secondary|   1782| 19|  jun|     no|\n",
      "| 28|secondary|   1238| 14|  may|     no|\n",
      "| 27|secondary|   1596|  1|  sep|    yes|\n",
      "| 29|secondary|   1386| 28|  may|     no|\n",
      "| 29|secondary|   1135| 17|  feb|    yes|\n",
      "+---+---------+-------+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "bank_df = bank_df.filter(col(\"balance\")>1000)\\\n",
    "            .filter(col(\"balance\")<2000)\n",
    "bank_df = bank_df.where(col(\"education\").isin([\"primary\", \"secondary\"])).filter(col(\"age\")<30)\n",
    "bank_df = bank_df.filter(bank_df[\"marital\"]=='married')\n",
    "bank_df = bank_df.select(\"age\", \"education\", \"balance\", \"day\", \"month\", \"deposit\")\n",
    "bank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PARTITIONS: 5\n",
      "Partition 0: 4 records\n",
      "[Row(age='27', education='secondary', balance='1293', day='8', month='apr', deposit='yes'), Row(age='26', education='secondary', balance='1595', day='15', month='jun', deposit='yes'), Row(age='28', education='secondary', balance='1595', day='9', month='sep', deposit='yes'), Row(age='28', education='secondary', balance='1020', day='28', month='may', deposit='no')]\n",
      "Partition 1: 4 records\n",
      "[Row(age='27', education='secondary', balance='1303', day='21', month='may', deposit='no'), Row(age='28', education='secondary', balance='1086', day='20', month='apr', deposit='yes'), Row(age='23', education='secondary', balance='1309', day='3', month='jun', deposit='no'), Row(age='29', education='secondary', balance='1180', day='17', month='apr', deposit='yes')]\n",
      "Partition 2: 3 records\n",
      "[Row(age='26', education='secondary', balance='1595', day='2', month='mar', deposit='no'), Row(age='26', education='secondary', balance='1417', day='6', month='jun', deposit='no'), Row(age='27', education='secondary', balance='1595', day='29', month='dec', deposit='yes')]\n",
      "Partition 3: 3 records\n",
      "[Row(age='24', education='secondary', balance='1222', day='20', month='apr', deposit='no'), Row(age='28', education='secondary', balance='1137', day='6', month='feb', deposit='no'), Row(age='25', education='secondary', balance='1782', day='19', month='jun', deposit='no')]\n",
      "Partition 4: 4 records\n",
      "[Row(age='28', education='secondary', balance='1238', day='14', month='may', deposit='no'), Row(age='27', education='secondary', balance='1596', day='1', month='sep', deposit='yes'), Row(age='29', education='secondary', balance='1386', day='28', month='may', deposit='no'), Row(age='29', education='secondary', balance='1135', day='17', month='feb', deposit='yes')]\n"
     ]
    }
   ],
   "source": [
    "print_partitions(bank_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Search using SQL language in Spark  <a class=\"anchor\" id=\"parallel_search_sparksql\"></a>\n",
    "#### Spark SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "df.createOrReplaceTempView(\"df_sql\")\n",
    "filter_sql = spark.sql('''\n",
    "  SELECT \n",
    "      age,education,balance,day,month,deposit\n",
    "  FROM \n",
    "      df_sql\n",
    "  WHERE \n",
    "      balance between 1000 and 2000\n",
    "  AND education in ('secondary','primary')\n",
    "  AND age < 30\n",
    "  AND marital = 'married'\n",
    "''')\n",
    "# filter_sql.explain()\n",
    "filter_sql.collect()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "FIT5202 - Getting started with Apache Spark.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
