{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-ANW9UaDDg-"
   },
   "source": [
    "##  Machine Learning with Spark \n",
    "\n",
    "Classification Using Decision Tree, Random Forest, and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW29BEFcDDhE"
   },
   "source": [
    "### **Attributes**\n",
    "\n",
    "1. age (numeric)\n",
    "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4. education (categorical: 'primary', 'secondary', 'tertiary', 'unknown')\n",
    "5. default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6. balance : bank balance\n",
    "7. housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "8. loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "### Related with the last contact of the current campaign:\n",
    "9. contact: contact communication type (categorical: 'cellular','telephone','unknown')\n",
    "10. day: last contact day of the week (numerical: 1,2,...28,29,30)\n",
    "1. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "12. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "\n",
    "### Other attributes:\n",
    "12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14. previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success','unknown')\n",
    "\n",
    "### Output variable (desired target):\n",
    "16. deposit - has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIyCqEZ7DDhG"
   },
   "source": [
    "### Step 1: Data Loading and Preparation <a class=\"anchor\" name=\"data-preparation\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gkn7FrQ0DDhH"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_conf = SparkConf()\\\n",
    "            .setMaster(\"local[*]\")\\\n",
    "            .setAppName(\"ML-Classification\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "df = spark.read.csv('bank.csv', header = True, inferSchema = True)\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DqeYXFMPDDhI"
   },
   "outputs": [],
   "source": [
    "# First, save the category in the category columns list.\n",
    "categoryInputCols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "numericInputCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "categoryOutputCol = 'deposit'\n",
    "categoryCols = categoryInputCols+[categoryOutputCol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwZ48hZdDDhJ"
   },
   "source": [
    "### Step 2: Feature Engineering <a class=\"anchor\" name=\"feature-engineering\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TQPri0qtDDhL",
    "outputId": "bae512b0-2615-4265-f656-f0d88542782b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome', 'deposit']\n",
      "['job_index', 'marital_index', 'education_index', 'default_index', 'housing_index', 'loan_index', 'contact_index', 'poutcome_index', 'label']\n"
     ]
    }
   ],
   "source": [
    "### Convert categorical columns\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Define the output columns\n",
    "outputCols=[f'{x}_index' for x in categoryInputCols]\n",
    "outputCols.append('label')\n",
    "\n",
    "print(categoryCols)\n",
    "print(outputCols)\n",
    "\n",
    "# Create the index values for categorical values\n",
    "# Initialize StringIndexer (use inputCols and outputCols)\n",
    "inputIndexer = StringIndexer(inputCols=categoryCols, outputCols=outputCols)\n",
    "\n",
    "# Call the fit and transform() method to get the encoded results \n",
    "df_indexed = inputIndexer.fit(df).transform(df)\n",
    "\n",
    "# # Display the output, only the output columns\n",
    "# df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NtqYtoJrDDhM",
    "outputId": "f861ad8f-c458-4f98-b2ba-8544f9d48b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|       job|job_index|       job_vec|\n",
      "+----------+---------+--------------+\n",
      "|    admin.|      3.0|(11,[3],[1.0])|\n",
      "|    admin.|      3.0|(11,[3],[1.0])|\n",
      "|technician|      2.0|(11,[2],[1.0])|\n",
      "+----------+---------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# input columns for OHE are all output columns from StringIndexer except label\n",
    "inputCols_OHE = [x for x in outputCols if x!='label']\n",
    "outputCols_OHE = [f'{x}_vec' for x in categoryInputCols]\n",
    "\n",
    "#Define OneHotEncoder with the appropriate columns\n",
    "encoder = OneHotEncoder(inputCols=inputCols_OHE,\n",
    "                        outputCols=outputCols_OHE)\n",
    "\n",
    "model = encoder.fit(df_indexed)\n",
    "# Call fit and transform to get the encoded results\n",
    "df_encoded = model.transform(df_indexed)\n",
    "# # Display the output columns\n",
    "# df_encoded.show(3)\n",
    "# print('\\n')\n",
    "df_encoded.select(\"job\", \"job_index\", \"job_vec\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6lGcSoR1DDhN",
    "outputId": "856e39ce-cf72-47b9-cb76-e1b3d1e9cf54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['job_vec', 'marital_vec', 'education_vec', 'default_vec', 'housing_vec', 'loan_vec', 'contact_vec', 'poutcome_vec', 'age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(30,[3,11,13,16,1...|\n",
      "|(30,[3,11,13,16,1...|\n",
      "|(30,[2,11,13,16,1...|\n",
      "|(30,[4,11,13,16,1...|\n",
      "|(30,[3,11,14,16,1...|\n",
      "|(30,[0,12,14,16,2...|\n",
      "|(30,[0,11,14,16,2...|\n",
      "|(30,[5,13,16,18,2...|\n",
      "|(30,[2,11,13,16,1...|\n",
      "|(30,[4,12,13,16,1...|\n",
      "|(30,[3,12,13,16,1...|\n",
      "|(30,[1,11,13,16,1...|\n",
      "|(30,[0,11,14,16,2...|\n",
      "|(30,[1,12,14,16,1...|\n",
      "|(30,[2,12,14,16,1...|\n",
      "|(30,[0,14,16,18,2...|\n",
      "|(30,[1,12,15,16,1...|\n",
      "|(30,[4,11,13,16,1...|\n",
      "|(30,[3,11,13,16,1...|\n",
      "|(30,[3,13,16,20,2...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols are all the encoded columns from OHE plus numerical columns\n",
    "inputCols=outputCols_OHE\n",
    "assemblerInputs = outputCols_OHE + numericInputCols\n",
    "print(assemblerInputs)\n",
    "\n",
    "# Define the assembler with appropriate input and output columns\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# use the asseembler transform() to get encoded results\n",
    "df_final = assembler.transform(df_encoded)\n",
    "\n",
    "# Display the output\n",
    "df_final.select('features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: (30,[3,11,13,16,17],[1.0,1.0,1.0,50.0,789.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr2F7wseDDhN"
   },
   "source": [
    "### Step 3: Pipeline API <a class=\"anchor\" name=\"pipeline\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Xg6vaXltDDhO",
    "outputId": "85097c41-c040-48f6-e137-fac7bc39001e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Pipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps.\n",
    "stage_1 = inputIndexer\n",
    "stage_2 = encoder\n",
    "stage_3 = assembler\n",
    "\n",
    "stages = [stage_1,stage_2,stage_3]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)\n",
    "\n",
    "#Choose only label and features to create a dataframe\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df_pipeline = df_pipeline.select(selectedCols)\n",
    "df_pipeline.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C7EsNVGDDhO"
   },
   "source": [
    "### Step 4: Train/Test Split <a class=\"anchor\" name=\"train-test\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wDetljkzDDhO",
    "outputId": "e770b60f-7ff6-468e-9276-9a766d5d2621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7858\n",
      "Test Dataset Count: 3304\n"
     ]
    }
   ],
   "source": [
    "# Divide data into train sets and test sets. \n",
    "# Seed is the value used to make the same data three times later\n",
    "train, test = df_pipeline.randomSplit([0.7, 0.3], seed = 2020)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3eKW_e7DDhP"
   },
   "source": [
    "## ML Classification Models <a class=\"anchor\" name=\"models\"></a>\n",
    "<hr />\n",
    "\n",
    "### Decision Tree <a class=\"anchor\" name=\"dt\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glvbj11tDDhP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Extracts the number of nodes in the decision tree and the tree depth in the model and stores it in dt.\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhTph-k2DDhP",
    "outputId": "37cc3de7-965b-4aa8-fcbc-aee21e2e6ecf"
   },
   "outputs": [],
   "source": [
    "dtPredictions = dtModel.transform(test)\n",
    "dtPredictions.select('features','label','prediction','probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGski5MmDDhQ"
   },
   "source": [
    "### Random Forest <a class=\"anchor\" name=\"rf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAzhPpApDDhQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "forestModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVuzXv7tDDhR",
    "outputId": "b4f77ea0-1c25-4b0a-9960-350827c9802d"
   },
   "outputs": [],
   "source": [
    "rfPredictions = forestModel.transform(test)\n",
    "rfPredictions.select('features','label','prediction','probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBvKALMQDDhR"
   },
   "source": [
    "### Logistic Regression <a class=\"anchor\" name=\"lr\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOM8vrwfDDhR"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create an initial model using the train set.\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10,regParam=0.3,elasticNetParam=0.7)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya6memAoDDhR",
    "outputId": "ac700e4d-a33f-4059-ce7a-53f9d9b7db9d"
   },
   "outputs": [],
   "source": [
    "lrPredictions = lrModel.transform(test)\n",
    "lrPredictions.select('features','label','prediction','probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8tGIeABDDhS"
   },
   "source": [
    "## Model Evaluation <a class=\"anchor\" name=\"model-evaluation\"></a>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-Nq2Sv-DDhT"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(predictions):\n",
    "    # Calculate the elements of the confusion matrix\n",
    "    TN = predictions.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = predictions.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = predictions.filter('prediction = 0 AND label <> prediction').count()\n",
    "    FP = predictions.filter('prediction = 1 AND label <> prediction').count()\n",
    "    \n",
    "    # calculate metrics by the confusion matrix\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1 = 2/((1/recall)+(1/precision))\n",
    "    return accuracy,precision,recall,f1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olgIqVN1DDhT",
    "outputId": "80dc7f82-982b-4308-b6e4-ce0957c5dd6d"
   },
   "outputs": [],
   "source": [
    "print('Logistic regression:',compute_metrics(lrPredictions))\n",
    "print('Decision Trees:',compute_metrics(dtPredictions))\n",
    "print('Random Forest:',compute_metrics(rfPredictions))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week6_Classification Algorithms sample solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
