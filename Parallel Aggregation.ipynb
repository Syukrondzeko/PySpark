{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcWYqAxbFIee"
   },
   "source": [
    "##  Parallel Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [SparkContext and SparkSession](#one)\n",
    "* [Parallel Aggregation](#two)\n",
    "    * [Group By](#groupby)        \n",
    "    * [Sort By](#sortby)    \n",
    "    * [Distinct](#distinct)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iChz1a-tk7aP"
   },
   "source": [
    "<a class=\"anchor\" name=\"one\"></a>\n",
    "## Import Spark classes and create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsQiS58Ak7aQ"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "master = \"local[*]\"\n",
    "app_name = \"Parallel Aggregation\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "783QFsKyk7aV"
   },
   "source": [
    "<a class=\"anchor\" name=\"two\"></a>\n",
    "## Parallel Aggregation\n",
    "\n",
    "Now we will implement basic aggregation functionalities and visualise the parallelism embedded in Spark as well as the execution plan and functions done to perform these kind of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVENTS INFO:\n",
      "Number of partitions: 10\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Height: string (nullable = true)\n",
      " |-- Weight: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- NOC: string (nullable = true)\n",
      " |-- Games: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Season: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Sport: string (nullable = true)\n",
      " |-- Event: string (nullable = true)\n",
      " |-- Medal: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema_events = StructType([\n",
    "    StructField(\"ID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Sex\", StringType(), True),\n",
    "    StructField(\"Age\", StringType(), True),\n",
    "    StructField(\"Height\", StringType(), True),\n",
    "    StructField(\"Weight\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"NOC\", StringType(), True),\n",
    "    StructField(\"Games\", StringType(), True),\n",
    "    StructField(\"Year\", StringType(), True),\n",
    "    StructField(\"Season\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Sport\", StringType(), True),\n",
    "    StructField(\"Event\", StringType(), True),\n",
    "    StructField(\"Medal\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Sample Data for Athlete Events\n",
    "data_events = [\n",
    "    (\"1\", \"John Doe\", \"M\", \"24\", \"180\", \"80\", \"United States\", \"USA\", \"2000 Summer\", \"2000\", \"Summer\", \"Sydney\", \"Swimming\", \"100m Freestyle\", \"Gold\"),\n",
    "    (\"2\", \"Jane Smith\", \"F\", \"22\", \"165\", \"60\", \"Canada\", \"CAN\", \"2016 Summer\", \"2016\", \"Summer\", \"Rio\", \"Athletics\", \"Marathon\", \"Silver\"),\n",
    "    (\"3\", \"Emily White\", \"F\", \"27\", \"170\", \"70\", \"Great Britain\", \"GBR\", \"2014 Winter\", \"2014\", \"Winter\", \"Sochi\", \"Curling\", \"Curling Women's\", \"Bronze\"),\n",
    "    (\"4\", \"Emily Blue\", \"M\", \"29\", \"170\", \"70\", \"Great Britain\", \"GBR\", \"2014 Winter\", \"2014\", \"Winter\", \"Sochi\", \"Curling\", \"Curling Women's\", \"Bronze\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_events = spark.createDataFrame(data=data_events, schema=schema_events)\n",
    "\n",
    "# Repartition DataFrame as described\n",
    "df_events = df_events.repartition(10)\n",
    "\n",
    "df_events.createOrReplaceTempView(\"sql_events\")\n",
    "print(f\"EVENTS INFO:\")\n",
    "print(f\"Number of partitions: {df_events.rdd.getNumPartitions()}\")\n",
    "df_events.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fytB-sHek7ad"
   },
   "source": [
    "### Group By <a class=\"anchor\" name=\"groupby\"></a>\n",
    "This part contains a simple aggregation query. Look into the query plan and level of parallelism in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqM6wRhik7ae"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#### Aggregate the dataset by 'Year' and count the total number of athletes using Dataframe\n",
    "agg_attribute = 'Year'\n",
    "df_count = df_events.groupby(agg_attribute).agg(F.count(agg_attribute).alias('Total'))\n",
    "\n",
    "#### Aggregate the dataset by 'Year' and count the total number of athletes using SQL\n",
    "sql_count = spark.sql('''\n",
    "  SELECT year,count(*)\n",
    "  FROM sql_events\n",
    "  GROUP BY year\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0lO3gQ-k7ai",
    "outputId": "41efc5f2-6e28-40e2-8388-2d6906756b91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year='2014', Total=1),\n",
       " Row(Year='2016', Total=1),\n",
       " Row(Year='2000', Total=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort By <a class=\"anchor\" name=\"sortby\"></a>\n",
    "We can use orderBy operation to sort the dataframe based on some column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-------------+\n",
      "|Year|       Name|         Team|\n",
      "+----+-----------+-------------+\n",
      "|2000|   John Doe|United States|\n",
      "|2014|Emily White|Great Britain|\n",
      "|2016| Jane Smith|       Canada|\n",
      "+----+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.select('Year','Name','Team').orderBy(df_events.Year).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INsF2yY_k7an"
   },
   "source": [
    "### Distinct <a class=\"anchor\" name=\"distinct\"></a>\n",
    "This part contains a simple query to get the distinct values of one of the attributes and then sorting them by the same attribute in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUMVzlrkk7ar"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year='2000'), Row(Year='2014'), Row(Year='2016')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Get the distinct values for 'Year' in the dataset using Dataframe\n",
    "df_distinct_sort = df_events.select('Year').distinct().sort('Year', ascending=True)\n",
    "\n",
    "#### Get the distinct values for 'Year' in the dataset using SQL\n",
    "sql_distinct_sort = spark.sql('''\n",
    "  SELECT distinct Year\n",
    "  FROM sql_events\n",
    "  ORDER BY year\n",
    "''')\n",
    "df_distinct_sort.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----+\n",
      "|       Name|          Event|Year|\n",
      "+-----------+---------------+----+\n",
      "| Jane Smith|       Marathon|2016|\n",
      "|Emily White|Curling Women's|2014|\n",
      "| Emily Blue|Curling Women's|2014|\n",
      "|   John Doe| 100m Freestyle|2000|\n",
      "+-----------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.select('Name','Event','Year').sort('Year', ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "FIT5202 - Parallel Aggregation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
